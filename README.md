# LLM-based Surprisal Estimation

This project provides a suite of scripts to estimate word surprisal using Large Language Models (LLMs). The workflow is designed to preprocess text datasets, calculate surprisal for target words, and merge the results back into the original data, enriched with word frequency information.

This repository contains the processing pipeline used for six datasets. The comment in scripts within the `GECOCN` folder are provided in English.

See https://osf.io/35ke6 for the results based on 25 models.


## Setup and Installation

This project requires Python 3.10 and several specific libraries. It is highly recommended to use a virtual environment (like Conda) to manage dependencies. The following instructions are based on a system with **CUDA 12.4**.

### 1. Create a Conda Environment

First, create and activate a new conda environment named `myenv` with Python 3.10.

```bash
conda create -n myenv python=3.10 -y
conda activate myenv
```

### 2. Install JAX with CUDA Support

This project uses JAX. Install the specific version compatible with your CUDA driver. The following command is for CUDA 12.

```bash
# For CUDA 12.x
pip install "jax[cuda12_pip]==0.4.27" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```
*Note: If you have a different CUDA version or are using a CPU, please refer to the [official JAX installation guide](https://github.com/google/jax#installation) for the correct command.*

### 3. Install PyTorch and Transformers

Next, install PyTorch, Transformers, and other essential libraries for running the language models.

```bash
pip install torch transformers einops transformers_stream_generator sentencepiece tiktoken protobuf accelerate
```

### 4. Install Data Science Libraries

Finally, install libraries required for data manipulation and analysis.

```bash
pip install pymc numpyro pandas matplotlib seaborn arviz
```

## Workflow & Usage

The process is divided into several key steps, from model downloading to final data consolidation.

### Step 0: Packages

```bash
# CUDA  12.4
conda create -n myenv python=3.10 -y
pip install "jax[cuda12_pip]==0.4.27" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install pymc numpyro pandas matplotlib seaborn arviz
pip install torch transformers einops transformers_stream_generator sentencepiece tiktoken protobuf accelerate
```

### Step 1: Download Models

First, download the necessary language models from the Hugging Face Hub. The `EN_download_models.sh` and `EN_download_models_bin.sh` is provided for this purpose.

Before running, you may want to modify the script to specify the exact model names you wish to download.

To run the script, first make it executable and then run it from your terminal:

```bash
chmod +x EN_download_models.sh
./EN_download_models.sh
```

### Step 2: Data Preprocessing

This step prepares your raw text data for surprisal calculation. The scripts for this are located within each dataset's respective folder (e.g., `GECOCN`).

1.  **`cut-GECOCN.py`**
    *   **Purpose**: This script processes the raw dataset to define context boundaries for each target word. It annotate the truncation of context where necessary (`cut-col`) and assigns a unique `word_id` to every word.

2.  **`textprocessing-GECOCN.py`**
    *   **Purpose**: Using the output from the `cut` script, this script formats the data by explicitly defining the context and the target word for each instance. It generates a file that is ready to be used for surprisal estimation.

### Step 3: Surprisal Estimation

The core surprisal calculation is handled by the main script in the root directory.

*   **`surprisal_v6.py`**
    *   **Purpose**: This is the main file for estimating surprisal. It takes the preprocessed files as input and leverages a pre-trained language model to calculate the surprisal value for each target word.
    *   **Usage**: Detailed instructions on how to run the script, including command-line arguments and required parameters, are documented within the comments at the top of the `surprisal_v6.py` file. You may want to modify `model_en.txt` for the models that you intend to use.
 
### Step 3.5: (Optional) Convert Surprisal to Probability

If your analysis requires probability values instead of surprisal, you can convert them using the `merge.py` script. The conversion is based on the formula: `Probability = 2^(-Surprisal)`.

*   **`merge.py`**
    *   **Purpose**: Transforms the calculated surprisal values into their corresponding probability values.
    *   **Usage**: Run this script on the output files of single model estimation generated by `surprisal_v6.py`.
    *   **NOTE**: The current `preprocessing` file is used for this step. You may want to modify the `preprocessing` scipt if you do not want to convert the surprisal to probability: file name of the output; dependent variable name across the `preprocessing` script.

### Step 4: Post-processing and Data Merging

After the surprisal values have been calculated, the following scripts are used to finalize the dataset.

1.  **`mean.py`** 
    *   **Purpose**: Calculates the average surprisal value from the output of the estimation script.
    *   **Note**: Remember to check the path in the script if you see errors.

2.  **`preprocessing-GECOCN.py`**
    *   **Purpose**: This final script merges the calculated surprisal estimations back into the original dataset. It also supplements the data by adding word frequency information derived from the SUBTLEX corpus.
    *   **Note**: Remember to check the path in the script if you see errors.

## Using Your Own Dataset

This processing pipeline is designed to be adaptable for other datasets. To use it with your own data, follow these general steps:

1.  **Create a Dataset Folder**: Create a new directory for your dataset (e.g., `my_dataset/`).
2.  **Prepare Your Data**: Place your raw data files inside this new folder.
3.  **Identify Dependencies**: To understand which specific files are required for your dataset, inspect the file import statements at the beginning of the `cut.py` and `preprocessing.py` scripts in the example folders. Ensure your folder contains the necessary input files.
4.  **Adapt Scripts**: Copy and adapt the processing scripts (`cut.py`, `textprocessing.py`, `preprocessing.py`) for your dataset's specific format and file names.

## Contributor
Xiaoxuan Li: sdlixiaoxuan@gmail.com

Faculty of Psychology, Beijing Normal University

Feel free to contact me if you need some help :D

2025.11.14
